# -*- coding: utf-8 -*-
"""
Created on Thu Jul  8 19:18:48 2021
@author: C√©line & Thao 
"""

import pandas as pd
import numpy as np
import datetime 
import time
import matplotlib.pyplot as plt 
import seaborn as sns
import streamlit as st
from PIL import Image
import requests
import io
from io import StringIO
import os

from sklearn import linear_model
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score
from sklearn import neighbors
from sklearn import datasets
from scipy.spatial.distance import cdist
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import silhouette_score

##------- ALL PAGE 
st.set_page_config(page_title="UpPySales App",page_icon="üéØ",layout="centered",initial_sidebar_state="expanded")

page = st.sidebar.radio(label="Menu", options = ['1Ô∏è‚É£ Pr√©sentation',  '2Ô∏è‚É£ Segmentation visiteurs', '3Ô∏è‚É£ Clustering'])

### LOGO
@st.cache
def img():
    urllogo = "https://raw.githubusercontent.com/Pythaon/uppysales_/main/uppysales_s.png"
    image = Image.open(requests.get(urllogo, stream=True).raw)
    return image
    
image = img()
    
st.image(image, width=None)

st.title("""**Analyse de l'activit√© de e-commerce**""")

##------- IMPORT DES DATASETS
@st.cache
def load_data1():
    url_events="http://spowls.net:449/projet/datasets/events.csv"
    s_events=requests.get(url_events).content 
    events=pd.read_csv(io.StringIO(s_events.decode('utf-8')))
    return events

### Df_all sample random 30% 
@st.cache
def load_data2():
    url_df_all="http://spowls.net:449/projet/datasets/df_all_sample30.csv"
    s_df_all=requests.get(url_df_all).content 
    df_all=pd.read_csv(io.StringIO(s_df_all.decode('utf-8')))
    return df_all
  
events = load_data1()
df_all = load_data2()

##------- PAGE PR√âSENTATION
if page == '1Ô∏è‚É£ Pr√©sentation':
        
    
    st.header("""**1Ô∏è‚É£ Pr√©sentation**""")  
             
    st.write(""" 
             
            Le jeu de donn√©es est un journal d'activit√©, il contient donc des timestamps, 
            des actions de visiteurs sur le site pendant 139 jours.
            Nous connaissons l'activit√© des visiteurs: l‚Äôarticle (itemid) visit√©, 
            le/les mis au panier, ou achet√©(s), le prix de l'item et sa disponibilit√©.
            La nature des articles est anonymis√©e (nous ne connaissons pas le contenu du site).
             
             """)
    
    ### -- 
    st.subheader("""La table principale est la table event:""") 

    st.dataframe(events.head())
    
    st.write(""" 
             
            La table est tr√®s √©troite avec seulement 5 variables, de plus le jeu de donn√©es est
            d√©s√©quilibr√© sur la variable **transactionid** avec moins de **1%** des enregistrements 
            qui concernent des transactions.
             
             """)
   
  
    fig, ax = plt.subplots(figsize=(4,3))
    sns.countplot(df_all['event'], ax=ax)
    #plt.figure(figsize=(1,3))
    st.pyplot(fig)
    
    ### -- 
    
    st.write(""" 
             
           Afin de donner un peu de largeur au dataset, nous avons ajout√© des variables calcul√©es 
           (ex :dur√©es entre les √©v√®nements de vue, mise au panier, achat, prix, disponibilit√© des items).
             
             """)
   
  
    st.subheader("""La table ci-dessous est un sample random de 30% du dataset principal retravaill√©""")   
    
    st.dataframe(df_all.head())

    
##------- PAGE SEGMENTATION VISITEURS 

if page == '2Ô∏è‚É£ Segmentation visiteurs':
        
    def main():
    
        st.header('**2Ô∏è‚É£ Segmentation visiteurs**')
        st.write("""
         La premi√®re analyse du jeu de donn√©es nous sugg√®re que le site attire beaucoup de visiteurs
            mais r√©alise tr√®s peu de transactions. Nous pensons que pour √©viter le biais du survivant, il
            faut √©viter de se focaliser uniquement sur les comportements des clients qui ont r√©alis√© une
            transaction, mais aussi observer les interactions qui n'ont pas donn√© lieu √† une transaction
            pour pouvoir sugg√©rer des changements au propri√©taire de ce site e-commerce pour
            am√©liorer son taux de conversion.
            
            Ainsi notre analyse RFM prend en compte les √©l√©ments suivants:
                
                
            ‚óè R - R√©cence : date de derni√®re action sur le site
        
            
            ‚óè F - Fr√©quence : nombre d'action sur le site
            
            
            ‚óè M - Monetary : Montant des transactions s‚Äôil y en a eu
        
        """)

        
        @st.cache
        def seg_vis(df_all):
        
            rev_par = df_all.groupby(['transactionid', 'visitorid']).agg({'price':'sum'}).reset_index()
            rev_par = rev_par.groupby('visitorid').agg({'price':'sum'}).reset_index()
            rev_par = rev_par.rename(columns={'price':'rev par visitor'})
            
            rev_par.head(2)
            
            #Fusion avec le dataset principal
            df_sco=df_all.merge(rev_par, how='left', on='visitorid')
            
            
            
              
            # Grouper
            visitors = df_sco.groupby(df_sco['visitorid'], as_index = False).agg({'timestamp':np.max,'event':'count',
                                                                              'ev_view':'sum','ev_addtocart':'sum',
                                                                              'ev_transaction':'sum', 'itemid':'count',
                                                                              'rev par visitor':np.max,'categoryid':'count',
                                                                              'parentid':'count',})
            
            # Formater
            visitors.visitorid=visitors.visitorid.astype('object')
            
            visitors["date_auj"] = datetime.date.today()
            visitors["date_max"] = df_sco['timestamp'].max()
            
            # Assignation des colonnes F et M du score RFM
            visitors =visitors.rename(columns={'event':'F'})
            visitors =visitors.rename(columns={'rev par visitor':'M'})
            
            
            visitors['date_max']=pd.to_datetime(visitors['date_max'])
            visitors['timestamp']=pd.to_datetime(visitors['timestamp'])
            
            # Cr√©ation de la colonne R - √©cart de la date de derni√®re action avec la derni√®re date du dataset
            visitors["R"] = visitors.date_max - visitors.timestamp
            
            # Suppression des colonnes inutiles
            visitors= visitors.drop(['timestamp', 'ev_view', 'ev_addtocart', 'ev_transaction', 'itemid',
               'categoryid', 'parentid', 'date_auj', 'date_max'], axis=1)
            
            # Changement de type de la colonne R qui doit √™tre un nombre de jour qui permet les calculs
            visitors['R']=visitors['R'].dt.days
            visitors['R']=visitors['R'].astype('int')
            
            # Statistiques des variables de la table visitors
            #visitors.describe().round(2).T
            
            # Analyse sur M hors 0 (pour pouvoir segmenter les acheteurs en fonction de leur valeur, le M = 0 qui
            # correspond √† un chiffre d'affaires de 0‚Ç¨ des visiteurs sans achat √©tant pr√©pond√©rant celui-doit √™tre
            # exclu pour voir la r√©poartition de M pour les acheteurs et en faire le score))
            
            M_hors_0 = visitors[(visitors['M']!=0)]
            M_hors_0['M'].describe().round(2)
            
            # Cr√©ation des scores grace aux quartiles
            # F : choix de Q2, Q3, max
            # Les valeurs retenues pour M sont 18 720 (Q1), 51 480  (Q3), 3 278 784 (max)
            r_bins = [-1, 35, 66, 137]
            f_bins = [0, 1, 2, 2465]
            m_bins = [0, 18720, 51480, 3278784]
            visitors['R_score'] = pd.cut(visitors['R'], r_bins, labels = ["3", "2", "1"])
            visitors['F_score'] = pd.cut(visitors['F'], f_bins, labels = ["1", "2", "3"])
            visitors['M_score'] = pd.cut(visitors['M'], m_bins, labels = ["1", "2", "3"])
               
            
            # Remplacement des NAN par valeur "0"
            # remplacement: ajouter une nouvelle cat√©gorie puis fill na avec cette nouvelle cat√©gorie (https://stackoverflow.com/questions/32718639/pandas-filling-nans-in-categorical-data/44633307)
            visitors['M_score'] = visitors['M_score'].cat.add_categories(0).fillna(0)
            
            # Concat√©nation du score RFM
            visitors["RFM_SCORE"] = visitors['R_score'].astype(str) + visitors['F_score'].astype(str) + visitors['M_score'].astype(str)
            
               
            # cr√©ation du dictionnaire de r√©f√©rence score RFM => Segment
            dico = {'110':'0_Non int√©ress√©','111':'1_Nouveaux ','112':'1_Nouveaux ','113':'1_Champions','120':'0_Curieux','121':'1_Nouveaux ','122':'1_Nouveaux ','123':'1_Champions','130':'0_Int√©ress√©','131':'1_Nouveaux ','132':'1_Nouveaux ','133':'1_Champions','210':'0_Non int√©ress√©','211':'1_A fid√©liser','212':'1_A fid√©liser','213':'1_Champions','220':'0_Curieux','221':'1_A fid√©liser','222':'1_A fid√©liser','223':'1_Champions','230':'0_Int√©ress√©','231':'1_A fid√©liser','232':'1_A fid√©liser','233':'1_Champions','310':'0_Non int√©ress√©','311':'1_A retenir','312':'1_A retenir','313':'1_Champions en risque','320':'0_Curieux','321':'1_A retenir','322':'1_A retenir','323':'1_Champions en risque','330':'0_Int√©ress√©','331':'1_A retenir','332':'1_A retenir','333':'1_Champions en risque'}
            
            visitors['RFM_SCORE'] = visitors['RFM_SCORE'].astype('str')
            
            # Ajout de la colonne segment
            visitors['segment'] = visitors['RFM_SCORE'].map(dico)
            
            
            # Compte du nombre de visitorid par segment
            graph = visitors.groupby("segment").agg({'visitorid':'count'}).reset_index(0)
            somme=graph['visitorid'].sum()
            graph['percent'] = ((graph['visitorid']/somme)*100).round(2)
        
            return(graph)
        
        
        graph = seg_vis(df_all)       
        
        st.table(graph)
        
        status = st.radio("Selectionner la vue: ", ('Ensemble des visiteurs', 'Focus acheteurs'))
        
        if (status == 'Ensemble des visiteurs'):
       
            
            def status_1(graph):
            
                # Repr√©sentation graphique des segments
                fig, ax = plt.subplots()
                sns.set(rc={'figure.figsize':(20.7,15)})
                g = sns.catplot(x='segment',y='percent',kind='bar',data=graph, height=6, aspect= 3)
                g.ax.set_ylim(0,80)
                for p in g.ax.patches:
                    txt = str(p.get_height().round(0)) + '%'
                    txt_x = p.get_x()
                    txt_y = p.get_height()
                    g.ax.text(txt_x,txt_y,txt);
                    
                st.pyplot(g)
                
            status_1(graph)
            
            st.write("""
                         L'allure de la repr√©sentation graphique n'√©chappe pas √† la nature d√©s√©squilibr√©e du dataset.
            
                Elle permet d'observer n√©amoins qu'on peut trouver 3 groupes de visiteurs sans achat :
                
                **> Non int√©ress√©:**
                
                cette cat√©gorie la plus importante concerne les visiteurs qui sont entr√©s sur le site et sortis imm√©diatement (pas de transaction, nombre d'action =1)
                
                - soit les clients sont entr√©s par erreur, ce qui peut sugg√©rer que le site est mal r√©f√©renc√© remonte en r√©sultat dans une recherche autre que celle de l'objet du site.
                
                - Soit les clients ont voulu voir un article qui n'est pas disponible et sont ressortis imm√©diatement.
                
                Analyse compl√©mentaire: parmis les visites des non int√©ress√©s quel % concernait des produits non disponibles?
                
                **> Int√©ress√©s et curieux:**
                
                sont des visiteurs qui ont parcouru le site, les interess√©s ayant fait deux actions ou plus. Ceci montre que 14% des visiteurs du site √©taient int√©ress√©s par ce qui s'y vend mais non pas √©t√© jusqu'√† la transaction. Ces clients sont un vivier potentiel de conversion.
                
                Analyse compl√©mentaire: quels sont les attribus des produits consult√©s qui ont pu limiter la conversion?
                         """)
        
             
        else:
        
            def status_2(graph):
            
                graph_ach = graph.copy()
                
                # Filtre sur les cat√©gories commen√ßant par 1
                # Extraction du premier caract√®re de la colonne segment
                graph_ach['rep']=graph_ach['segment'].str[0]
                # Filtrage
                graph_ach= graph_ach[(graph_ach['rep']=='1')]
                graph_ach= graph_ach.drop(['rep', 'percent'], axis=1)
            
                
                # Compte du nombre de visitorid par segment
                somme_ach=graph_ach['visitorid'].sum()
                graph_ach['percent'] = ((graph_ach['visitorid']/somme_ach)*100).round(2)
            
                
                # Repr√©sentation graphique des segments
                sns.set(rc={'figure.figsize':(20.7,15)})
                
                g = sns.catplot(x='segment',y='percent',kind='bar',data=graph_ach, height=6, aspect= 3)
                g.ax.set_ylim(0,50)
                
                for p in g.ax.patches:
                    txt = str(p.get_height().round(0)) + '%'
                    txt_x = p.get_x()
                    txt_y = p.get_height()
                    g.ax.text(txt_x,txt_y,txt);
                    
                st.pyplot(g)
                
            status_2(graph)
            
            
        
            st.write("""
                    **40%** des acheteurs sont des acheteurs r√©cents, les identifier permet de pouvoir leur adresser une campagne de bienvenue et de remerciement. Nous pouvons √©galement tenter de les fid√©liser.
                    
                    **17%** des acheteurs sont des "champions", en terme de chiffre d'affaires, mais attention 7% d'entre eux sont en risque car n'ont pas eu d'activit√© r√©cente.
                    La segmentation "champion" appelle a un traitement plus d√©di√© de ce segment de client√®le.
                    
                    **20%** des acheteurs sont "√† retenir", leur chiffre d'affaires et de moyen √† faible, mais ils no'nt pas eu d'activit√© r√©cente, ils sont √† relancer.
                    
                    **17%**des clients n'ont pas de caract√©ristiques particul√®res de recence ou de chiffre d'affaires, ils peuevnt √™tre cibl√©s par une campagne de communication plus g√©n√©raliste pour les faire revenir sur le site et les fid√©liser.
                    
                    Il conviendrait de rejouer cette segmentation sur un jeu de donn√©es avec un journal d'activit√© du site sur une plus longue p√©riode, afin d'observer les comportements dans le temps de mani√®re plus pr√©cise.
                    
                    Ici, sur une p√©riode de 4 mois il y a **40% des acheteurs** qui sont cons√©d√©r√©s comme nouveaux, il est possible qu'une partie d'entre eux avaient fait des achats dans les semaines avant le d√©but du relev√©.
                                     """)
           
    main()    
    

   
             
##------- PAGE CLUSTERING 

if page =='3Ô∏è‚É£ Clustering':
    
    st.header("**3Ô∏è‚É£ Clustering**")
    items = df_all.groupby(df_all['itemid'], as_index = False).agg({'event':'count', 'ev_view':'sum','ev_addtocart':'sum', 'ev_transaction':'sum', 'price':'mean', 'categoryid':'mean', 'parentid':'mean'})

    # on retire les lignes sans prix
    items = items.dropna(axis = 0, how='all', subset=['price'])
    # suppression des variables cat√©gorielles
    items = items.drop(['categoryid', 'parentid', 'itemid', 'event'], axis = 1)
            
    # Normalisation
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    items_sc = scaler.fit(items)
    items_sc = scaler.transform(items)

    #fonction qui lance les mod√®les

    st.markdown("""
                Nous allons tester les mod√®les suivants:""")
                    
    models = ['Kmeans', 'Clustering Mixte kmeans & ACH']
                  
    choix_modele = st.radio("", options=models)
        
    def main2():

        if choix_modele ==models[0]:
            
            @st.cache(suppress_st_warning=True)
            def cluster(graph1):
                from scipy.spatial.distance import cdist
                from sklearn.cluster import KMeans
                # Liste des nombre de clusters
                
                range_n_clusters = np.arange(2,10)
                
                # Initialisation de la liste de distortions
                distortions = []
            
            
                # Calcul des distortions pour les diff√©rents mod√®les
                for n_clusters in range_n_clusters:
                    # Initialisation d'un cluster ayant un pour nombre de clusters n_clusters
                    cluster = KMeans(n_clusters = n_clusters)
                    # Apprentissage des donn√©es suivant le cluster construit ci-dessus
                    cluster.fit(items)
                    # Ajout de la nouvelle distortion √† la liste des donn√©es
                    distortions.append(sum(np.min(cdist(items_sc, cluster.cluster_centers_, 'euclidean'), axis=1)) / np.size(items, axis = 0))
                
                # Courbe du coude
                fig_coude, ax = plt.subplots()
                #plt.figure(figsize=(5, 6))
                plt.plot(range_n_clusters, distortions)
                plt.xlabel('Nombre de Clusters K')
                plt.ylabel('Distortion (WSS/TSS)')
                plt.title('M√©thode du coude affichant le nombre de clusters optimal')
                
                st.pyplot(fig_coude)
                
            cluster()
    main2()
    
    def main3():
    
        if choix_modele==models[1]:
            
            @st.cache
            def test(a,b):
                c = a + b
                return c
            a=5
            b=7
            res=test(a,b)
            
            st.write("""test = """, res)
    
    main3()
                
